<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<!--Converted with LaTeX2HTML .95.3 (Nov 17 1995) by Nikos Drakos (nikos@cbl.leeds.ac.uk), CBLU, University of Leeds -->
<HTML>
<HEAD>
<TITLE>A Hypothetical Reasoning Framework for Natural Language Processing</TITLE>
<meta name="description" value="A Hypothetical Reasoning Framework for Natural Language Processing">
<meta name="keywords" value="art">
<meta name="resource-type" value="document">
<meta name="distribution" value="global">
<LINK REL=STYLESHEET HREF="art.css">
</HEAD>
<BODY LANG="EN">
 <H1 ALIGN=CENTER>A Hypothetical Reasoning Framework for Natural Language Processing</H1>
<P ALIGN=CENTER><STRONG>
   <B>Andrew Fall</B> <BR>
	Logic Programming Group, <BR>
	School of Computing Science, <BR>
	Simon Fraser University, <BR>
	Burnaby, B.C. V5A-1S6 <BR>
	<TT>fall@cs.sfu.ca</TT> <BR>
         <BR>
   <B>Paul Tarau</B> <BR>
	D&#233;partement d'Informatique, <BR>
	Universit&#233; de Moncton <BR>
	Moncton, Canada E1A-3E9 <BR>
	<TT>tarau@info.umoncton.ca</TT> <BR>
         <BR>
   <B>Veronica Dahl</B> <BR>
	Logic Programming Group, <BR>
	School of Computing Science, <BR>
	Simon Fraser University, <BR>
	Burnaby, B.C. V5A-1S6 <BR>
	<TT>veronica@cs.sfu.ca</TT>	
    </STRONG></P><P>
<P ALIGN=CENTER><STRONG></STRONG></P><P>
<P>
<H3 CLASS=ABSTRACT>Abstract:</H3>
<P CLASS=ABSTRACT><EM>We introduce a framework for natural language processing
based on linear and intuitionistic assumptions,
a new type of logic grammars (HAGs), and a sparse term encoding
of sorts used to implement semantic taxonomies.
Our framework is able to infer type relationships
as well as deal with backtrackable
state information in a principled yet highly efficient way.
<P>
<EM>Intuitionistic</EM> and <EM>linear</EM> implications scoped over the current continuation
allow us to follow given branches of the computation under hypotheses that 
disappear when and if backtracking takes place. 
Unlike previous similar
frameworks, ours maintains high-level descriptiveness through the use
of hidden <EM>multiple accumulators</EM>. 
This new technique offers the equivalent
of EDCGs [<A HREF="art.html#edcg">Van89</A>] without the need for a preprocessor.
<P>
A type hierarchy of concepts
is automatically constructed and efficiently encoded using
<EM>sparse terms</EM> from basic
definitions given by the user, and can be consulted, also in high level
ways, in order to draw inferences re. class belongings and inherited
properties.
<P>
<B>Keywords:</B> 
 
<EM>natural language processing, logic grammars,
linear (intuitionistic) implication, hypothetical assumptions, 
alternative DCG implementations,
Hidden Accumulator Grammars,
backtrackable destructive assignment,
sparse logical terms, taxonomies.</EM>
</EM></P><P>
<H1><A NAME="SECTION00010000000000000000">Introduction</A></H1>
 <A NAME="intro">&#160;</A>
<P>
The idea of using linear and intuitionistic logic for natural language
processing is not new. In [<A HREF="art.html#pareschiextendingiclp90">PM90</A>],
for instance, a method
was proposed for handling relativization by temporarily augmenting a 
Prolog grammar with a missing noun phrase rule, only during the
parse of a relative clause. Thus, in ``the review that Amy completed'',
for instance, the relative clause can be viewed as a sentence in which
the object noun phrase required for ``completed'' is missing (to be equated
with the antecedent ``the review''). 
While allowing for empty noun phrases anywhere in the sentence would lead to overgeneration (e.g., sentences with 
missing subjects) or need to be controlled through cumbersome extra arguments
of multiple-headed grammar rules (see e.g. [<A HREF="art.html#DA89NSERC">AD89</A>]),
the linear 
logic approach offers a clean and effective formulation, even though it
admittedly has some problems (e.g. it admits erroneous sentences in which a
relative clause has no missing noun phrase, as in *``the house that jack built
the cottage''). These problems were later addressed by Miller and Hodas in
a linear logic framework [<A HREF="art.html#Hodas92">Hod92a</A>, <A HREF="art.html#jlp44">HM92</A>]
in which a relative clause rule
can be expressed as follows (modulo notation changes for standard readability):
<P>
<PRE>  rel([that|L1],L2):- np(Z,Z) -o s(L1,L2).
</PRE>
<P>
This rule expresses that a relative clause can be recognized in a string 
starting with ``that'' if we can recognize a sentence in the remainder
(L1) of the input string, by hypothesizing a missing noun phrase only
during the derivation of that sentence. Since the hypotheses must be
used, relative clauses with no empty np are correctly avoided.
<P>
Two things should be noticed about this rule:
<P>
<UL><LI> the input and output strings (L1 and L2) MUST be explicitly stated, thus forcing us
to regress to pre-DCG grammar formulations, and
<P>
<LI> the missing noun phrase can be anywhere in the relative sentence, so that
specifying a particular kind of relativization (on the subject as in ``the
book that arrived today'', on the object as in ``the book that I read today'',
or on a complement as in ``the student that I gave a book to''), and the semantic
relation of the missing noun phrase with its antecedent, might not be
very straightforward to achieve.
</UL>
<P>
The idea of using type information in natural language processing is also
not a new one. Within logic programming contexts it has the specific
attraction of being representable by incomplete terms, i.e., terms
containing logical variables which can be further specified dynamically
through unifications, so that operations such as type intersection can
reduce to little more than unification. 
This has been exploited for instance in [<A HREF="art.html#Dahl91">Dah91</A>, <A HREF="art.html#AitKaci93">AKP93</A>].
More recently, <EM>sparse terms</EM> [<A HREF="art.html#Fall95a">Fal95</A>] have been developed as a flexible and efficient representation in which incomplete information can be easily extended through unification.
Sparse terms have also been studied as means to efficiently encode taxonomies in a form that is amenable to computing operations such as <EM>greatest lower bound</EM> and <EM>subsumption</EM> [<A HREF="art.html#Fall94">Fal94</A>].
Taxonomic encoding automates the management of partially ordered information, and is a fundamental component of -term management in LIFE [<A HREF="art.html#AitKaci93">AKP93</A>].
<P>
Because these hierarchies are invisibly managed, we can view them
as another case of hypothetical reasoning (albeit of a 
different nature than that allowed by linear or intuitionistic implication),
in that it provides automatic reasoning on the hypotheses we usually make
about type hierarchies and their inheritance properties.
<P>
The use of multiple accumulators in natural language processing is a new idea which was proposed in [<A HREF="art.html#Fall95b">FDT95</A>], applied to the problem of co-specification resolution.
We believe that they are a valuable tool for logic grammars which generalize Definite Clause Grammars (DCGs) to Hidden Accumulator Grammars (HAGs).
<P>
In section <A HREF="art.html#hypothetical">2</A>, we introduce our hypothetical reasoning framework which integrates linear and intuitionistic logic, HAGs and virtual type hierarchies.
We then show, by examples in sections <A HREF="art.html#disc">3</A>, <A HREF="art.html#dist">4</A> and <A HREF="art.html#reso">5</A>, how this framework can simplify the treatment of many computational linguistics problems,  while simultaneously facilitate more readable grammars.
Our examples focus on the problems of world modeling, long distance dependencies and co-specification resolution, although we feel that many other problems could be elegantly tackled using our framework.
Section <A HREF="art.html#related">6</A> then discusses
related work, and section <A HREF="art.html#future">7</A> gives some areas for future research.
<P>
<H1><A NAME="SECTION00020000000000000000">A hypothetical reasoning based framework for NL processing </A></H1>
 <A NAME="hypothetical">&#160;</A>
<P>
The basic components of our framework are the following:
<P>
<OL><LI> linear and intuitionistic implications scoped over the current
continuation
<P>
<LI> hidden multiple accumulator grammars (HAGs)
<P>
<LI> reasoning on virtual type hierarchies deduced from a user's definitions.
</OL>
<P>
We next shortly describe each of these in turn.
We refer to the companion paper [<A HREF="art.html#TDF95a">TDF95</A>] for
the details of their design and implementation in
BinProlog [<A HREF="art.html#Tarau95BinProlog">Tar95</A>].
<P>
<H2><A NAME="SECTION00021000000000000000">Linear and intuitionistic implication</A></H2>
<P>
Intuitionistic assumei/1 adds temporarily a clause usable
in later proofs. Such a clause can be used an indefinite
number of times, like asserted clauses, except that it
vanishes on backtracking. 
Its scoped version <tex2html_verb_mark><tex2html_verb_mark> or <tex2html_verb_mark><tex2html_verb_mark>
makes <tex2html_verb_mark><tex2html_verb_mark> or the set of clauses found in <tex2html_verb_mark><tex2html_verb_mark>
available only during the proof of Goal and corresponds to
the intuitionistic implication of Prolog.
While assumption A in the intuitionistic
implication <tex2html_verb_mark><tex2html_verb_mark> is usable only within the proof of
B, after assumei(A), A is usable within the current AND-continuation.
<P>
Linear assumel/1 adds temporarily a clause usable <EM>at most once</EM>
in later proofs. This assumption also vanishes on backtracking.
Its scoped version <tex2html_verb_mark><tex2html_verb_mark> <A NAME="tex2html1" HREF="#197"><IMG ALIGN=BOTTOM ALT="gif" SRC="/opt/tools/latex2html-95.3/icons//foot_motif.gif"></A>
or <tex2html_verb_mark><tex2html_verb_mark>
makes <tex2html_verb_mark><tex2html_verb_mark> or the set of clauses found in <tex2html_verb_mark><tex2html_verb_mark>
available only during the proof of Goal. 
Both vanish on backtracking.
Linear implication and assumption 
can be seen as implicitly existentially quantified,
i.e. the original copy is used in further unifications,
while intuitionistic implication and assumption behave
like asserts with the usual copying semantics, both on use and
on definition.
Although assumed clauses can be called directly, it is good
programming practice (and slightly more
efficient) to call them through the use of BinProlog's <TT>assumed/1</TT>
builtin which will look directly to assumed code without
attempting to find first a statically compiled definition.
<P>
<H4><A NAME="SECTION00021010000000000000">Programmer enforced weakening rule</A></H4>
<P>
The <EM>weakening</EM> rule in linear logic requires every assumption
to be eventually used. For the natural language world we
intend to model, when assumptions range over the current
continuation, this requirement seems too strong, except for
the well-known situation of handling relatives through the use
of gaps [<A HREF="art.html#Hodas92">Hod92a</A>].
<P>
Therefore, we have left <EM>weakening</EM> 
in the hand of the programmer
who can simply specify (by using for instance, negation
as failure)
that, at a given point, the set of
left over assumptions should be empty.
<P>
<H2><A NAME="SECTION00022000000000000000">Hidden Accumulators</A></H2>
<P>
When we want to avoid any search and enforce some
sequentiality on the use of linear
assumptions (a stack discipline, for instance)
we will use the observation of [<A HREF="art.html#TDF95a">TDF95</A>]
that accumulator processing (EDCGs) is a special instance
of hypothetical reasoning with linear assumptions.
BinProlog's Hidden Accumulator Grammars are a variant of EDCGs
which do not require preprocessing.
They have been implemented in C and are
accessible through the following set of built-ins:
<P>
<PRE>dcg_connect/1  % works like 'C'/3 with 2 invisible arguments
dcg_def/1   % sets the first invisible DCG argument
dcg_val/1   % retrieves the current state of the DCG stream
dcg_tell/1  % focuses on a given DCG stream
dcg_telling/1 % returns the number of the current DCGs stream
</PRE>
<P>
On top of them it is possible to define a `multi-stream'
phrase/3 construct,
<P>
<PRE>dcg_phrase(DcgStream, Axiom, Phrase)
</PRE>
<P>
that
switches to the appropriate <TT>DcgStream</TT> and uses <TT>Axiom</TT>
to process or generate <TT>Phrase</TT>.
We refer to the companion paper [<A HREF="art.html#TDF95a">TDF95</A>]
for their specification in term of linear assumptions.
<P>
<H2><A NAME="SECTION00023000000000000000">Sparse Term Taxonomies</A></H2>
<P>
<A NAME="taxonomy">&#160;</A>
<P>
Hierarchical, or taxonomic, information is pervasive in many areas of inquiry.
In natural language processing, taxonomies have been used for semantic type hierarchies, in which selectional constraints can be imposed, and for word class hierarchies, in which syntactic constraints can be specified [<A HREF="art.html#Fall95b">FDT95</A>, <A HREF="art.html#PS87">PS87</A>].
An analysis of the use of taxonomies in unification-based grammar formalisms which exploit typed feature structures is given in [<A HREF="art.html#Carp92">Car92</A>].
<P>
The main operations on taxonomies are <EM>greatest lower bound</EM>, <EM>least upper bound</EM> and <EM>subsumption checking</EM>, in addition to the specification of inheritable properties.
For in-depth coverage of how large taxonomies can be managed in order to make these operations efficient, see [<A HREF="art.html#AitKaci89">AKBLN89</A>, <A HREF="art.html#Fall94">Fal94</A>].
<P>
We build on research in [<A HREF="art.html#Fall95b">FDT95</A>] to extend the application of hierarchical information in natural language processing.
As an example, consider the discourse <EM>``The dog saw the mailman. He was barking.''</EM>.
We can construct a hierarchy of semantic types, in which mailmen are a subset of people, and the inheritable property that the agent of <EM>barking</EM> is usually a dog.
This information would effectively rule out <EM>the mailman</EM> as referring to the same individual as <EM>he</EM> unless there was additional contextual or world knowledge to believe that the mailman might bark.
<P>
<H3><A NAME="SECTION00023100000000000000">Sparse Logical Terms</A></H3>
<P>
There are two uses of variables in Herbrand terms: as place holders and for co-reference.
Since Herbrand terms have fixed arity, anonymous variables serve only to fill unspecified positions.
Sparse terms [<A HREF="art.html#Fall95a">Fal95</A>] by default have variable arity and so remove the need for anonymous variables.
The implicit position information of Herbrand terms is lost, and so must be explicitly stated.
A similar situation arises in sparse matrix representation, on which sparse terms are modeled.
As an example, the term: a(b(__,_,X),_,_,d(X,_,e(_,_),_)) is represented as a sparse term by: <tex2html_verb_mark><tex2html_verb_mark>, according to the following definition:
<P>
<P><P>
<P>
One of the advantages of sparse terms is the ease with which they may be extended.
We describe below some these extensions that are useful for natural language processing.
<P>
<DL ><DT>Binding Arity:
<DD>
Since arity is variable, there is no one-to-one correspondence between sparse and ordinary terms. 
For example, the following terms correspond to the sparse term <tex2html_verb_mark><tex2html_verb_mark>: f(a), f(a,_), f(a,_,_), f(a(_),_), ... 
We can explicitly bind the arity of a term by extending part (iii) of our definition to allow functors of the form <tex2html_verb_mark><tex2html_verb_mark> where <tex2html_verb_mark><tex2html_verb_mark> is the arity of the functor.
As an example, the term f(a(_),_,b) would be completely represented by <tex2html_verb_mark><tex2html_verb_mark>.
<P>
<DT>Anonymous Functors:
<DD>
An interesting variation allows terms to specify only those argument positions which are occupied, but not record the functor or atom in that position.
This is useful to maintain uncertainty or to avoid repeating functor information for terms that will later be unified.
The sparse term <tex2html_verb_mark><tex2html_verb_mark> represents a term of arity at least 4 and two subterms in the second and fourth positions.
<P>
<DT>Attribute Indices:
<DD>
The indexing in Prolog terms is implicitly numeric.
Since sparse terms maintain indexing explicitly, a trivial extension allows atomic rather than just numeric indices.
This provides some of the functionality of -terms [<A HREF="art.html#AitKaci93">AKP93</A>] and permits implementation of attribute-value matrices, or feature structures, as used in several computational linguistic systems (e.g. [<A HREF="art.html#Carp92">Car92</A>, <A HREF="art.html#PS87">PS87</A>]).
A predicate can be provided to access the value of an attribute, or a sequence of attributes.
<P>
<DT>Disjunctive Functors:
<DD>
<P>
Thus far, we have two levels of certainty regarding a functor symbol: either it is unknown (i.e. it may be any atomic symbol) or it is known. 
Between these extremes lies a range of increasingly focused information as to the actual symbol. 
That is, we may know that it is one of a set of possible symbols. When this set has cardinality one, the symbol is full specified. 
We name such functors <EM>disjunctive</EM> and represent them with set notation. 
For example, the term <tex2html_verb_mark><tex2html_verb_mark> may be used to represent a computer system whose model type is either a MacSE or a MacII and with either 1, 2, 4 or 8 KB of memory.
<P>
<DT>Disjunctive Terms:
<DD>
<P>
For similar reasons, we may wish to maintain uncertainty at the term level (either for entire terms, or for subterms within a term).
This is also accomplished using the set notation.
As an example, the term <tex2html_verb_mark><tex2html_verb_mark> may be used to represent the word <i>bark</i> as a first person, second person, or third person plural verb.
This can be later constrained to, e.g., third person, as in:
<P>
<PRE>| ?- sortDef(bark,S,_), unifyST([person-third],S,T).

S = {bark~[person-{first;second},sem-s_bark~[agent-s_dog]];
     bark~[person-third,number-plural,sem-s_bark~[agent-s_dog]]},
T = bark~[person-third,number-plural,sem-s_bark~[agent-s_dog]] ?
</PRE>
<P>
<DT>Generalized Coreference:
<DD>
For coreference, LIFE uses more generalized coreference labels than the named variables of Prolog. These labels can specify coreference between any two locations in the graphical representation, not just between leaves.
<P>
 </DL>
<P>
In order to synthesize these enhancements, we extend our sparse term definition:
<P>
<PRE><TT> 
SparseTerm 		 := 		 CorefLabel:ST <i>|</i> CorefLabel: <i>|</i> ST
<P>
		 		 <i>|</i> CorefLabel:{ST; ...;ST} <i>|</i> {ST; ...;ST} (where cardinality is <i>&gt;</i> 1)
<P>
CorefLabel 		 := 		 String
<P>
ST			 := 		 Functor <i>|</i> Functor<tex2html_verb_mark><tex2html_verb_mark>ArgList <i>|</i> ArgList
<P>
Functor 		 := 		 Atom <i>|</i> Atom/Arity <i>|</i> /Arity <i>|</i> {Atom; ...;Atom}
<P>
 		 		 <i>|</i> {Atom, ..., Atom}/Arity (where cardinality is <i>&gt;</i> 1)
<P>
ArgList 		 := 		 [ ] <i>|</i> [Argument <i>|</i> ArgList]
<P>
Argument 		 := 		 Index-SparseTerm
<P>
Index 		 := 		 String (i.e. an attribute indicating the argument position).
<P>
</TT></PRE>
<P>
In order to use sparse terms, the predicate <tex2html_verb_mark><tex2html_verb_mark> unifies two input terms and produces the unified term <i>T</i>.
We also provide predicates for antiunification and subsumption checking.
<P>
Our representation shares some commonality with the -terms in LIFE [<A HREF="art.html#AitKaci93">AKP93</A>], in particular attribute indexing, unbound arity and coreference labels, but it also differs in several respects.
Sparse terms deviate from -terms for binding arity, anonymous functors and disjunction.
Another significant difference is that our representation is intended as an enhancement to Prolog systems, not as a replacement.
<P>
<H3><A NAME="SECTION00023200000000000000">Virtual Type Hierarchies</A></H3>
<P>
We have implemented predicates that permit the construction of hierarchies, in which each node of the hierarchy has associated with it a sort and a sparse term that encodes the default properties for entities of that type.
These predicates provide much of the functionality of LIFE constrained sorts [<A HREF="art.html#AitKaci93">AKP93</A>].
The top node in the hierarchy is <i>top</i> (i.e. ), and may not be modified.
<P>
The predicate <tex2html_verb_mark><tex2html_verb_mark> adds information that <i>Subsort</i> is a subsort of <i>Supersort</i>, provided that the partial order structure of the hierarchy is not violated.
This is equivalent to the ``<tex2html_verb_mark><tex2html_verb_mark>'' predicate in LIFE.
The default properties of the supersort are unified with prior properties of the subsort (if it had any) using what we call <EM>c-unification</EM>.
In c-unification, one of the terms is <EM>dominant</EM> and the other is <EM>subordinate</EM>.
During unification, if a conflict arises (in which ordinary unification would fail), only the information in the dominant term is kept.
This allows non-monotonic property inheritance.
When specifying subtyping, the pre-existing properties of a sort dominate those of the new supersort.
<P>
Properties are associated with sorts using the <tex2html_verb_mark><tex2html_verb_mark> predicate.
This is similar to the ``<tex2html_verb_mark><tex2html_verb_mark>'' predicate in LIFE.
Properties are specified as sparse terms, and are c-unified with the existing properties of the sort.
New properties are assumed to dominate previously defined properties.
<P>
There are several ways to access property terms associated with sorts.
Sorts are stored in predicates of the form <tex2html_verb_mark><tex2html_verb_mark>, which may be directly accessed.
In order to increase time efficiency, the Property term will contain all inherited properties, but to save on space and to allow for recursive properties, sorts within properties are not expanded.
A fully expanded property term can be obtained using the predicate <tex2html_verb_mark><tex2html_verb_mark>, but this will not terminate on cyclic properties.
<P>
To handle recursive properties, we implemented <EM>lazy expansion</EM> of sort properties. 
LIFE also does a lazy expansion of terms, but in a slightly different way than here.
As soon as an attribute with an unexpanded sort as value is accessed, the subterm is expanded using the sort's property term.
To illustrate, consider the following definitions, the first of which states that every person has a mother and birthdate:
<P>
<PRE>subsort(person, top).
set_property(person, [mother-person, birthdate]).

subsort(joe, person).
set_property(joe, [mother-anne~[mother-betty]]).
</PRE>
<P>
This builds a simple hierarchy of people, in which all people have a mother who is a person.
Clearly, fully expanding the person sort is not possible.
Instead, the property terms of ``person'' and ``joe'' are:
<P>
<PRE>| ?- sortDef(person,S,T).

S = person~[mother-person,birthdate],
T = [top] ? 

| ?- sortDef(joe,S,T).

S = joe~[mother-anne~[mother-betty~[mother-person,birthdate],birthdate],
         birthdate],
T = [person] ?
</PRE>
<P>
As can be seen, only a minimal amount of expansion is performed.
We must expand the leaf ``betty'' since the only connection between ``betty'' and the sort ``person'' is through the property specified for ``joe''.
Since ``betty'' is not a defined sort, if we don't expand during the property specification, we lose this connection.
<P>
Predicates are also provided to access and set values along particular chains of attributes.
For example, suppose we wish to get the ``mother of the mother'' attribute of some property term.
This is done through a projection predicate <tex2html_verb_mark><tex2html_verb_mark>, where the attribute path is specified by a list of attributes:
<P>
<PRE>| ?- sortDef(joe,S,_), projectST([mother, mother], S, X).

S = joe~[mother-anne~[mother-betty~[mother-person,birthdate],birthdate],
         birthdate],
X = betty~[mother-person,birthdate] ?
</PRE>
<P>
Setting the value at the end of an attribute path is done analogously to unification, using the predicate <tex2html_verb_mark><tex2html_verb_mark>, as in the following examples:
<P>
<PRE>| ?- sortDef(joe,S,_), setProjectST([mother,mother], jane, S,X).

S = joe~[mother-anne~[mother-betty~[mother-person,birthdate],birthdate],
         birthdate],
X = joe~[mother-anne~[mother-jane~[mother-person,birthdate],birthdate],
         birthdate] ?
</PRE>
<P>
The use of these predicates for the construction and use of a sort hierarchy can be seen in Appendix I.
<P>
<H1><A NAME="SECTION00030000000000000000">NL processing as world-modeling</A></H1>
 <A NAME="disc">&#160;</A>
<P>
Realistic natural language analysis
cannot make abstraction of semantics
and pragmatics as programming languages
cannot fully make abstraction of their
run-time environments.
Computer-based discourse understanding 
(as it's human counterpart) is basically a form of
model-building. It involves
constant constraint-solving to
keep, at a given time,
only a manageable subset of (intended) models.
The task is harder, but similar to that of
compilers for programming languages.
<P>
Following the compiler analogy,
anaphora resolution parallels the process of
symbol resolution in linkers.
As in the case of dynamic linking, some of this
information is provided on the fly, and
unsolved references at a given point make sense as far as
they will be solved in the future.
In this case, anaphora and ambiguity are likely to
be resolved with an
<EM>abductive</EM> world-modeling process.
Each sentence of the discourse generates and possibly
consumes/solves various hypotheses and constraints.
The process is backtrackable although some
form of commit can be provided when common sense
reasoning allows it or imposes it.
<P>
The effect of every sentence on the `world-model' is described
with a set of <EM>pre-</EM> and <EM>post-</EM>conditions which are 
either <EM>used</EM> or
<EM>defined</EM> as facts during the analysis.
<P>
Various discourse fragments exhibit two distinct modes:
<EM>define</EM> and <EM>use</EM>.
<P>
Let us illustrate this with two sentences and a (subset of)
their post-conditions.
<P>
<PRE>`a dog bit john'

  define: bitten_by(a_dog,john)
</PRE>
<P>
<PRE>`the dog that bit john is smart'

  use: bitten_by(a_dog,john)
  define: smart(a_dog)
</PRE>
<P>
Suppose a sentence refers to an individual through
an anaphora (pronoun, definite article)
which is not yet defined in the discourse.
It is reasonable to delay resolving the referent
and abduce (make the hypothesis) that such
an individual exists. Information from the current
sentence will be used to generate constraints on the 
referent of the anaphora. When the anaphora is tentatively
solved, abduced constraints are used to eliminate
inconsistent interpretations.
<P>
A difficult problem is that of `dereferencing' a given
description. In the general case this points to
the (undecidable) problem of testing the identity
of two first-order objects. A way to prevent this from
the start is to <EM>abstract</EM> identity information,
so that this <EM>dereferencing</EM> become trivial.
This loss of information, to be tolerable, requires
a way to project back the abstract information into the concrete domain,
where the referent is additionally constrained by
the current state of the world.
<P>
<H1><A NAME="SECTION00040000000000000000">Describing long distance dependencies through HAGs</A></H1>
 <A NAME="dist">&#160;</A>
<P>
As mentioned in the Introduction, previous linear logic based approaches
to long distance dependencies, such as relativization, force us to go back to
explicitly coding the input and output string in every rule. 
By using the features of HAGs we can restore high level expressiveness. 
Consider the following rules, which describe simple sentences
relativizing on a verb's subject, object or complement. They construct
a three-branched quantification-based semantics.
<P>
<PRE>% build R from the subject's representation (or quantified variable) X 
% and the verb phrase's representation VP
sent(R) :- np(X,VP,R), vp(X,VP).

np(X,VP,VP):- proper_name(X).

% build the scope NP from the representation N1 of the head and that of the
% relative clause
np(X,VP,R):- det(X,NP,VP,R), noun(X,N1), rel(X,N1,NP).

% consume the missing np's antecedent (hypothesized by the second rule for
% rel), and equate it with the variable introduced by the missing np
np(X,VP,VP):- assumed(antecedent(X)).	

det(X,NP,VP,a(X,NP,VP)) :- {a}.
det(X,NP,VP,the(X,NP,VP)):- {the}.

% build a three-branched quantification from a variable X,
% a scope NP and a restriction VP.
noun(X,woman(X)):- {woman}.
noun(X,title(X)):- {title}.
proper_name(john):- {john}.

rel(X,X).  		% empty relative clause

% having found a relative pronoun; find a sentence assuming
% X as the antecedent of a missing np
rel(X,N,and(N,R)):- {that}, antecedent(X) -: sent(R).

vp(X,P) :- intrans_vb(X,P).
vp(X,P):- trans_vb(X,Y,P1), np(Y,P1,P).
vp(X,P):- bitrans_vb(X,Y,Z,P1), np(Y,P1,P2), np(Z,P2,P).

intrans_vb(X,won(X)):- {won}.
trans_vb(X,Y,saw(X,Y)):- {saw}.
bitrans_vb(X,Y,Z,gave(X,Y,Z)):- {gave}.

{W}:-dcg_connect(W). % syntactic sugar for dcg_connect
</PRE>
<P>
The difference with usual DCGs is that only <EM>terminals</EM>
act as <EM>state transformers</EM>. <EM>Nonterminals</EM>
carry no existential variables and are implemented by ordinary
predicates. Output-construction, carried out in this example explicitly,
can be left as well to a second DCG-stream, while a third stream
can be used as a stack accumulating information for
future use e.g. in anaphora resolution.
For debugging purposes as well as for meta-interpretation
in general, the absence of explicit (single or multiple) DCG
arguments is an advantage.
<P>
<H1><A NAME="SECTION00050000000000000000">Resolving co-specification using HAGs and hypothetical assumptions</A></H1>
 <A NAME="reso">&#160;</A>
<P>
One of the most difficult problems in language processing is that of
determining which entities co-specify. For instance, in 
``John photographed Mary, and then she photographed him'', we can
think of the proper names ``John'' and ``Mary'' as specifying entities
in the world, and of the pronouns ``him'' and ``she''
as co-specifying the same entities. The notion of co-specification, first introduced in [<A HREF="art.html#Sidner81">Sid81</A>] avoids the pitfalls
of previous notions of antecedence, since a pronoun might follow rather than
precede its ``antecedent'' (as in ``When he sleeps, John snores''), or even be inexplicit
(as in ``I was caught speeding, but he only gave me a warning''). Notice also
that co-specification may be ambiguous (as in ``John photographed Tim, and
then he photographed Mary'') and may not respect syntactic agreement (as in
``If a student is admitted, they must satisfy the acceptance criteria'').
<P>
It is interesting that the extensions to logic grammars that we present in this paper admit two different
methodologies for resolving co-specification: the use of
multiple accumulators, and the use of linear and intuitionistic implication. The first one was investigated in [<A HREF="art.html#Fall95b">FDT95</A>], so we will only
briefly summarize it here. Then we shall introduce the hypothetical reasoning approach.
<P>
<H2><A NAME="SECTION00051000000000000000">Co-specification Resolution with Multiple Accumulators</A></H2>
<P>
<A NAME="matching">&#160;</A>
<P>
As a discourse is processed, specifiers can be marked for later consultation as potential co-specifiers through the use of accumulators.
To make co-specifier resolution as general as possible, all the information pertaining to each specifier encountered is accumulated in a stack.
During the resolution process, this stack contains a list of <EM>contextual</EM> referents, ordered temporally (i.e. potential co-specifiers derivable from the context/discourse).
As an example, after processing the first sentence of the discourse: ``John greeted Fred. He then greeted Cathy.'', the stack contains specifiers for John and Fred.
When the pronoun <EM>he</EM> is being resolved, it may potentially co-specify with either of these entities.
Constraints on matching can be used to select both possibilities, ordered according to preference criteria (so that <EM>John</EM> may be selected before <EM>Fred</EM>).
<P>
When processing a discourse, we may need to check if a definite specifier refers to one of the entities in the stack.
This searching process can be directed by providing a number of <EM>match skeletons</EM> that specify syntactic, semantic and contextual constraints that co-specification (i) <EM>must</EM> agree with (required), (ii) would <EM>preferably</EM> agree with (preferfed) and (iii) <EM>must not</EM> agree with (forbidden).
For the pronoun <EM>she</EM>, any co-specifier must be feminine and singular, whereas for <EM>this dog</EM>, the semantic type of any referent must be compatible with <EM>dog</EM>.
A theory of anaphoric co-specifiers may dictate that pronouns should try to match on focus [<A HREF="art.html#Sidner81">Sid81</A>] or case.
Since there may be multiple sets of preferred constraints, the matching process can be given a list of preferred skeletons, ordered by preference.
<P>
By determining the entire list of potential co-specifiers in one step, we avoid returning the same specifier twice.
Also, it may not be possible to express all the contextual and semantic constraints in the skeletal information given to the matching process, leading to two problems: (i) the list returned may not be ordered properly and (ii) it may contain superfluous co-specifiers.
Given this preliminary list, higher-level processes may then weed out impossible co-specifiers and impose additional ordering constraints to select from the remaining entities (i.e. to choose a preferred reading in the face of ambiguity).
A similar scheme is proposed in [<A HREF="art.html#Hobbs86">Hob86</A>], in which a lower level process generates a set of candidate referents for pronouns, and a higher level process selects one antecedent from this set using relative criteria.
<P>
To illustrate this process, suppose we use a theory of anaphora resolution that requires matching on gender and number, and prefers to match on case.
For the above discourse, we would return the list [<EM>John, Fred</EM>], since both satisfy the required gender and number constraints, but John matches on the nominative case.
This simple scheme fails, however, for the discourse: ``Cathy was talking with Fred. When Fred saw Sharon, she ignored him.''
When resolving <EM>she</EM>, the list returned is [<EM>Cathy, Sharon</EM>].
If we prefer to match on Sharon because she is mentioned later than Cathy, we can process the returned list, reversing the preference order based on the temporal ordering at the sentential or topic level.
Expressing general constraints such as temporal or topical proximity is better done on the resulting list rather than in the matching process,
particularly if such constraints involve relative comparisons among potential co-specifiers.
<P>
If a system also provides a set of semantic constraints (e.g. on the possible object and subject of a verb), these can be used to prune or order the potential co-specifiers.
To illustrate, consider the sentences: ``When John saw the dog, he was barking.'' and ``When John saw the dog, he was laughing.''
If our semantic constraints restrict the agent of barking to be a dog and the agent of laughing to be a person, then resolving <EM>he</EM> in both sentences unambiguously selects the correct co-specifier.
If we permit people to bark and dogs to laugh as unlikely, but possible scenarios, then the co-specification resolver will return both <EM>John</EM> and <EM>dog</EM> as potential co-specifiers for <EM>he</EM>. 
In the first case, the preferential order will be [<EM>dog, John</EM>], whereas in the second case it will be [<EM>John, dog</EM>].
Many of these semantic constraints can be made available, for example, in the virtual hierarchy.
<P>
The above scheme can automate many forms of co-specification.
In [<A HREF="art.html#Fall95b">FDT95</A>], we describe several extensions which improve the flexibility and generality of the process.
These extensions permit the specification of additional constraints to
control the matching process, and to derive new specifiers from mentioned
entities (e.g. to <EM>generalize</EM> a class from an instance, as in
``John is a marathon runner. They have a lot of endurance.''
or to <EM>specialize</EM> an instance from a class as in
``Harlequin ducks are beautiful. I saw one yesterday.'').
<P>
<H2><A NAME="SECTION00052000000000000000">Hypothetical reasoning vs. Hidden Accumulator Grammars</A></H2>
<P>
An alternative way of storing the information gleaned from the grammar and the noun phrases as they appear is through temporarily adding this information
as hypotheses ranging over the current continuation. Consulting it then
reduces to calling the predicate in which this information is stored.
<P>
To exemplify, let us consider a very simple grammar which retrieves
the list of all head nouns
as the semantic representation of its 
sentences. Consider the discourse:
<P>
<PRE>John walks.
Amy gives a computer to him.
</PRE>
<P>
Upon encountering/generating
the noun phrase ``John'', we can store the information
found in the lexical rule for ``John''  
with a linear implication.
<P>
When we then encounter, say, a pronoun, we can compare its syntactic 
features with those of the candidate co-specifiers (those stored in ``candidate'' hypotheses). We refer to Appendix I for the actual
implementation.
<P>
Notice that by scoping the implications over the current continuation,
the noun phrases of all previous sentences in the discourse are available for
resolving co-specification. Once a potential referent is found, it is
stored as a feature of the pronoun in question. Thus each pronoun will,
at the end of the analysis, carry the whole list of its potential
referents as a feature. User-defined criteria can then be used, as in
the multiple accumulator approach, to further refine the list of candidate 
co-specifiers.
<P>
We have defined the co-specifier predicate so that the representations of the
noun phrases are the only candidates allowed. It is interesting to point out
that in order to handle abstract co-specifiers [<A HREF="art.html#Asher93">Ash93</A>], such as events or propositions, all we have to
do is extend the definition so that other parts of a sentence can be
identified as possible specifiers as well.
<P>
<H1><A NAME="SECTION00060000000000000000">Related work</A></H1>
<P>
<A NAME="related">&#160;</A>
<P>
Existing work on Linear Logic based Natural Languages
processing [<A HREF="art.html#Hodas92">Hod92a</A>, <A HREF="art.html#jlp43">Hod92b</A>, <A HREF="art.html#jlp44">HM92</A>, <A HREF="art.html#pareschiextendingiclp90">PM90</A>] is mostly at sentence level, while, ours covers text level
constructs. This is made easy by using hypothetical assumptions
which range over the current continuation, instead of locally scoped
implications.
<P>
There is some commonality between our approach to co-specifier resolution and the pronoun anaphora approach in the public domain LIFE natural language analyzer.
In particular, both schemes employ a hierarchy of semantic types in order to impose selectional constraints (although the constraints themselves are specified using functions in the the LIFE program, instead of as inheritable properties as in our system), and both use accumulators to manage potential co-specifiers (although these are hand-coded in the LIFE program).
There are, however, several key differences in the approaches.
First, anaphora resolution in the LIFE program is based on antecedence, while our approach uses the more general and less problematic notion of co-specification [<A HREF="art.html#Sidner81">Sid81</A>].
Second, the resolution process in the LIFE program is fixed within the grammar rules: pronoun resolution simply searches the temporally ordered list of potential co-specifiers for the first match on gender, number and semantic type.
Our approach has been designed for flexibility: although some matching constraints may be specified in the grammar rules, most are specified lexically.
This allows a range of matching constraints and also permits matching on abstract entities, as in <EM>``John kicked Fred on Monday, and it hurt''</EM> [<A HREF="art.html#Asher93">Ash93</A>].
Finally, we have added a number of extensions to the matching process which allow the determination of implicit referents, as in <EM>generalization</EM> and <EM>specialization</EM>.
<P>
<H1><A NAME="SECTION00070000000000000000">Future work</A></H1>
 <A NAME="future">&#160;</A>
<P>
It would be interesting to explore
the interaction between our hypothetical
reasoning tools and constraint solvers. In practice this means porting
them to a finite domain constraint solver like 
WAMCC [<A HREF="art.html#CodDiazICLP95">CD95</A>], which also features global variables and
backtrackable destructive assignment.
<P>
Our tools are in an experimental stage, a tighter integration will follow.
The mapping from sparse terms to BinProlog's logical global attributed
variables will allow `compiling' them to WAM-code and ultimately to C,
therefore removing the remaining inefficiencies in some interpreted
operations.
We also intend to continue exploring the uses of sparse 
terms and taxonomies in natural language processing.
<P>
<H1><A NAME="SECTION00080000000000000000">Conclusion</A></H1>
<P>
<A NAME="conclusion">&#160;</A>
<P>
We have a presented a framework which uses various hypothetical reasoning techniques
implemented as backtrackable state information in some
typical natural language processing problems.
Their synergy is needed for reasons of expressiveness and flexibility.
The novelties are:
<UL><LI> linear implications scoped over the current continuation
(i.e. the remaining AND-branch of the resolution) allow extension beyond
individual sentences
<LI> although they can be seen as particular instances of linear assumptions [<A HREF="art.html#TDF95a">TDF95</A>]
Hidden Accumulator Grammars are an efficient and flexible tool, allowing enforcement of
chronological order on linear assumption (if needed) and a successful replacement for
DCGs, both on efficiency and ease of use grounds
<LI> HAGs and lazy expansion of taxonomies gives to a generic Prolog engine
some of the expressiveness of Life extensions (accumulators and sorts)
</UL>
Compared to previous frameworks based on Linear (Intuitionistic)
Logic, ours is portable and runs on top of generic Prolog systems.
This is a practical advantage over systems like Lolli or Prolog.
Backtrackable destructive assignment, when encapsulated in higher-level
constructs simplifies and possibly replaces widespread
idioms like DCGs while offering more powerful facilities in the form
of hypothetical assumptions and multiple accumulators.
This also reduces the need
for explicitly imperative constructs like <EM>assert</EM> and <EM>retract</EM>
in logic programming languages.
<P>
<P><A NAME="SECTIONREF"><H2>References</H2></A><P>
<DL COMPACT>
<DT><A NAME="DA89NSERC"><STRONG>AD89</STRONG></A><DD>
H. Abramson and V. Dahl.
<EM>Logic Grammars</EM>.
Symbolic Computation AI Series. Springer-Verlag, 1989.
<P>
<DT><A NAME="AitKaci89"><STRONG>AKBLN89</STRONG></A><DD>
H. A&#239;t-Kaci, R. Boyer, P. Lincoln, and R. Nasr.
Efficient implementation of lattice operations.
<EM>ACM Transactions on Programming Languages</EM>, 11(1):115-146,
  1989.
<P>
<DT><A NAME="AitKaci93"><STRONG>AKP93</STRONG></A><DD>
H. A&#239;t-Kaci and A. Podelski.
Towards a meaning of LIFE.
<EM>Journal of Logic Programming</EM>, 16(3/4):195, 1993.
<P>
<DT><A NAME="Asher93"><STRONG>Ash93</STRONG></A><DD>
N. Asher.
<EM>Reference to Abstract Objects in Discourse</EM>, volume 50 of <EM>
  Studies in Linguistics and Philosophy</EM>.
Kluwer, 1993.
<P>
<DT><A NAME="Carp92"><STRONG>Car92</STRONG></A><DD>
B. Carpenter.
<EM>The Logic of Typed Feature Structures</EM>.
Cambridge University Press, London, England, 1992.
<P>
<DT><A NAME="CodDiazICLP95"><STRONG>CD95</STRONG></A><DD>
Philippe Codognet and Daniel Diaz.
Compiling Prolog to C : the WAMCC system.
In <EM>Proceedings of the 12-th International Conference on Logic
  Programming</EM>, Yokohama, Japan, 1995. The MIT Press.
<P>
<DT><A NAME="Dahl91"><STRONG>Dah91</STRONG></A><DD>
V. Dahl.
Incomplete types for logic databases.
<EM>Applied Mathematical Letters.</EM>, 4(3):25-28, 1991.
<P>
<DT><A NAME="Fall94"><STRONG>Fal94</STRONG></A><DD>
A. Fall.
The foundations of taxonomic encoding.
<EM>Submitted to ACM Transactions on Programming Languages. Also
  available as Simon Fraser Universtity Technical Report SFU LCCR TR 94-20</EM>,
  1994.
<P>
<DT><A NAME="Fall95a"><STRONG>Fal95</STRONG></A><DD>
A. Fall.
Sparse logical terms.
<EM>To appear in Applied Mathematics Letters.</EM>, 1995.
<P>
<DT><A NAME="Fall95b"><STRONG>FDT95</STRONG></A><DD>
A. Fall, V. Dahl, and P. Tarau.
Resolving co-specification in contexts.
In <EM>Proc. of IJCAI Workshop on Context in Natural Language
  Processing</EM>, Montreal, Canada, 1995.
<P>
<DT><A NAME="jlp44"><STRONG>HM92</STRONG></A><DD>
J. Hodas and D. Miller.
Logic programming in a fragment of intuitionistic linear logic.
<EM>Journal of Information and Computation</EM>, 1992.
<P>
<DT><A NAME="Hobbs86"><STRONG>Hob86</STRONG></A><DD>
J. Hobbs.
Resolving pronoun references.
In <EM>Readings in Natural Language Processing</EM>, pages 339-352.
  Morgan Kaufmann Publishers, Inc., 1986.
<P>
<DT><A NAME="Hodas92"><STRONG>Hod92a</STRONG></A><DD>
J. Hodas.
Specifying Filler-Gap Dependency Parsers in a
  Linear-Logic Programming Language.
In Krzysztof Apt, editor, <EM>Logic Programming Proceedings of
  the Joint International Conference and Symposium on Logic
  programming</EM>, pages 622-636, Cambridge, Massachusetts London, England, 1992.
  MIT Press.
<P>
<DT><A NAME="jlp43"><STRONG>Hod92b</STRONG></A><DD>
J Hodas.
Specifying Filler-Gap Dependency Parsers in a Linear
  Logic Programming Language.
In Krzysztof Apt, editor, <EM>Proc. 1992 Joint International
  Conference and Symposium on Logic Programming</EM>, pages 622-636. MIT Press,
  1992.
<P>
<DT><A NAME="pareschiextendingiclp90"><STRONG>PM90</STRONG></A><DD>
R. Pareschi and D.A. Miller.
Extending definite clause grammars with scoping constructs.
In D.H.D. Warren and P. Szeredi, editors, <EM>7th Int. Conf. Logic
  Programming</EM>, pages 373-389, Jerusalem, Israel, 1990. MIT Press.
<P>
<DT><A NAME="PS87"><STRONG>PS87</STRONG></A><DD>
C. Pollard and I. Sag.
<EM>Information-Based Syntax and Semantics</EM>.
CSLI Lecture Notes No. 13. Stanford, CA, 1987.
<P>
<DT><A NAME="Sidner81"><STRONG>Sid81</STRONG></A><DD>
C. Sidner.
Focussing for Interpretation of Pronouns.
<EM>American Journal for Computational Linguistics</EM>, 7(4):217-231,
  1981.
<P>
<DT><A NAME="Tarau95BinProlog"><STRONG>Tar95</STRONG></A><DD>
Paul Tarau.
BinProlog 4.00 User Guide.
Technical Report 95-1, D&#233;partement d'Informatique, Universit&#233;
  de Moncton, February 1995.
Available by ftp from <EM>clement.info.umoncton.ca</EM>.
<P>
<DT><A NAME="TDF95a"><STRONG>TDF95</STRONG></A><DD>
Paul Tarau, Veronica Dahl, and Andrew Fall.
Backtrackable State with Linear Assumptions, Continuations
  and Hidden Accumulator Grammars.
Technical Report 95-2, D&#233;partement d'Informatique, Universit&#233;
  de Moncton, April 1995.
Available by ftp from <EM>clement.info.umoncton.ca</EM>.
<P>
<DT><A NAME="edcg"><STRONG>Van89</STRONG></A><DD>
Peter Van Roy.
A useful extension to Prolog's Definite Clause Grammar notation.
<EM>SIGPLAN notices</EM>, 24(11):132-134, November 1989.
</DL>
<P>
<H1><A NAME="SECTION000100000000000000000">Appendix I. A natural language analyzer/generator</A></H1>
<P>
<A NAME="storygen">&#160;</A>
<P>
<PRE>/*************************************************************************
Possible `stories' in generation mode:

?- go.

[john,walks,.,a,woman,laughs,.,a,woman,that,laughs,gives,a,computer,to,him]
...
[john,walks,.,a,man,buys,a,book,.,a,man,that,buys,a,book,gives,it,to,him]
**************************************************************************/

:-dynamic pred/1,gap/1.

go:- top(T),member(that,T),member(him,T),write(T),nl,fail.

top(Xs):-
    dcg_def(Xs),
    interesting-:interesting-:interesting-:text,
    dcg_val([]).

{W}:-dcg_connect(W). % syntactic sugar
+(W):-assumel(W). % syntactic sugar
*(W):-assumei(W).

% grammar

text:-interesting,sent,rest.

rest.
rest:-{.},text.

% the number of assumed pred/1 limits the amount of anaphoric references

sent:-sent(Args), + pred(Args), + pred(Args). 

sent([S,V]):-
    s(S),          should_be(S,kind,K_s),   should_be(S,case,subject),
    intrans_vb(V), should_be(V,agent,K_v),  subsumes_sort(K_v, K_s).
sent([S,V,O]):-
    s(S),        should_be(S,kind,KA_s),   should_be(S,case,subject),
    trans_vb(V), should_be(V,agent,KA_v),  subsumes_sort(KA_v, KA_s),
                 should_be(V,object,KO_v),
    o(O),        should_be(O,kind,KO_o),   subsumes_sort(KO_v, KO_o),
                 should_be(O,case,object).
sent([S,V,O,C]):-
    s(S),          should_be(S,kind,KS)_s,   should_be(S,case,subject),
    bitrans_vb(V), should_be(V,agent,KS_v),  subsumes_sort(KA_v, KA_s),
                   should_be(V,object,KO_v), should_be(V,co_agent,KC_v),
    o(O),          should_be(O,kind,KO_o),   subsumes_sort(KO_v, KO_o),
                   should_be(O,case,object),
    {to}, c(C),    should_be(C,kind,KC_c),   subsumes_sort(KC_v, KC_c),
                   should_be(C,case,object),
    S\==C.

s(X):-n(X).
o(X):-n(X).
c(X):-n(X).

v(N,X):-{X},should_be(X,args,N).

n(X):-gap([X|_]),!.
n(X):-pn(X).
n(X):-in(X).
n(X):-cn(X).
n(X):-cn(X),rel([X|_]).

pn(P):-{P},anaphora(P).
in(X):-{X},should_be(X,class,individual).
cn(X):-{a},{X},should_be(X,class,concept).
cn(X):-{the},{X},should_be(X,class,concept),known(X).

anaphora(P):- known(X),
  should_be(X,gender,G), should_be(P,gender,G), should_be(P,class,pronoun).

known(S):-pred([S|_]).
known(O):-pred([_,_,O|_]).

rel(Args):- rp, pred(Args),
  gap(Args) -: sent(Args).

rp:-{that}.

should_be(Word,Attribute,Value) :- projectST([Attribute],Word,Value).

% noun sorts
:- subsort(thing, top).
:- set_property(thing, [class-concept, gender-neutral, kind-thing]).
:- subsort(name, top).
:- set_property(name, [class-individual]).
:- subsort(animate, thing).
:- set_property(animate, [kind-animate]).
:- subsort(inanimate, thing).
:- set_property(inanimate, [kind-inanimate]).

:- subsort(person, animate).
:- subsort(woman, person).
:- set_property(woman, [gender-female]).
:- subsort(man, person).
:- set_property(man, [gender-male]).
:- subsort(john, name).
:- subsort(john, man).
:- subsort(book, inanimate).
:- subsort(computer, inanimate).
:- subsort(him, animate).
:- set_property(him, [case-object, class-pronoun, gender-male]).

% verb sorts
:- subsort(verb, top).
:- subsort(intransitive_verb, verb).
:- set_property(intransitive_verb, [args-1]).
:- subsort(transitive_verb, verb).
:- set_property(transitive_verb, [args-2]).
:- subsort(bitransitive_verb, verb).
:- set_property(bitransitive_verb, [args-3]).

:- subsort(walks, intransitive_verb).
:- set_property(walks, [agent-animate]).
:- subsort(laughs, intransitive_verb).
:- set_property(laughs, [agent-animate]).
:- subsort(buys, transitive_verb).
:- set_property(buys, [agent-animate, object-thing]).
:- subsort(gives, bitransitive_verb).
:- set_property(gives, [agent-animate, co-agent-animate, object-thing]).
</PRE>
<P>
<DL> <DT><A NAME="197">...version</A><DD>The use of 
<TT>-:</TT>
instead of the usual 
<TT>-o</TT> 
comes from the fact that in Prolog
an operator mixing alphabetic and
special characters would require quoting in infix position.
Also, since our implication differs semantically from
usual linear implication, it seems reasonable
to denote it differently.
<PRE>
</PRE> </DL>
<BR> <HR>
<P><ADDRESS>
<I>Paul Tarau <BR>
Sun Jan 14 14:31:06 AST 1996</I>
</ADDRESS>
</BODY>
</HTML>
